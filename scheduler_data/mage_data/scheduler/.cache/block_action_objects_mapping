{"block_file": {"data_exporters/export_data.py:data_exporter:python:export data": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.snowflake import Snowflake\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_snowflake(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a Snowflake warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#snowflake\n    \"\"\"\n    table_name = 'your_table_name'\n    database = 'your_database_name'\n    schema = 'your_schema_name'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Snowflake.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            table_name,\n            database,\n            schema,\n            if_exists='replace',  # Specify resolution policy if table already exists\n        )\n", "file_path": "data_exporters/export_data.py", "language": "python", "type": "data_exporter", "uuid": "export_data"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/outgoing_firefly.py:data_exporter:python:outgoing firefly": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.snowflake import Snowflake\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_snowflake(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a Snowflake warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#snowflake\n    \"\"\"\n    table_name = 'your_table_name'\n    database = 'your_database_name'\n    schema = 'your_schema_name'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Snowflake.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            table_name,\n            database,\n            schema,\n            if_exists='replace',  # Specify resolution policy if table already exists\n        )\n", "file_path": "data_exporters/outgoing_firefly.py", "language": "python", "type": "data_exporter", "uuid": "outgoing_firefly"}, "data_loaders/ingest_data.py:data_loader:python:ingest data": {"content": "import pandas as pd\nimport requests\nfrom datetime import datetime\nimport uuid\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.snowflake import Snowflake\nfrom os import path\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport gc\n\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Pipeline de backfill con streaming - exporta cada chunk inmediatamente\n    Capa Bronze: mantiene columnas originales + metadatos de ingesta\n    \n    Par\u00e1metros:\n    - service: 'yellow', 'green', 'taxi_zones', etc.\n    - year: A\u00f1o a procesar\n    - months: Lista de meses [1,2,3] o None para todos\n    - chunk_size: Filas por chunk (default: 1000000)\n    - force_reload: Sobrescribir datos existentes (default: False)\n    \"\"\"\n\n    print(f\"DEBUG kwargs completos: {kwargs}\")\n    print(f\"DEBUG year en kwargs: {'year' in kwargs}\")\n    print(f\"DEBUG valor de year: {kwargs.get('year', 'NO ENCONTRADO')}\")\n    \n    service = kwargs.get('service', 'yellow')\n    year = int(kwargs.get('year',2015))\n    months = kwargs.get('months', None)\n    database = get_secret_value('SNOWFLAKE_DATABASE')\n    schema = get_secret_value('SNOWFLAKE_SCHEMA')\n    chunk_size = int(kwargs.get('chunk_size', 1000000))\n    force_reload = kwargs.get('force_reload', False)\n    \n    # Normalizar months a lista de enteros\n    if months is None:\n        months = list(range(1, 13))\n    elif isinstance(months, int):\n        months = [months]\n    elif isinstance(months, str):\n        # Si viene como string \"[1,2,3]\" o \"1,2,3\"\n        months = [int(m.strip()) for m in months.strip('[]').split(',')]\n    elif isinstance(months, list):\n        # Convertir cada elemento a int por si vienen como strings\n        months = [int(m) for m in months]\n    \n    batch_run_id = str(uuid.uuid4())\n    \n    execution_date = kwargs.get('execution_date')\n    if execution_date:\n        if isinstance(execution_date, datetime):\n            batch_timestamp = execution_date.strftime('%Y-%m-%d %H:%M:%S')\n        else:\n            batch_timestamp = str(execution_date)\n    else:\n        batch_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \n    print(f\"=\" * 80)\n    print(f\"BACKFILL STREAMING - {service.upper()} {year}\")\n    print(f\"Meses: {months}, Chunk size: {chunk_size:,}, Batch ID: {batch_run_id[:8]}\")\n    print(f\"=\" * 80)\n    \n    results = {\n        'batch_run_id': batch_run_id,\n        'service': service,\n        'year': year,\n        'batch_timestamp': batch_timestamp,\n        'months_attempted': len(months),\n        'months_successful': 0,\n        'months_skipped': 0,\n        'months_failed': 0,\n        'months_gap': 0,\n        'total_rows_loaded': 0,\n        'monthly_results': []\n    }\n    \n    for month in months:\n        print(f\"\\n[{month:02d}] Procesando {service} {year}-{month:02d}\")\n        \n        month_result = process_month_streaming(\n            service=service,\n            year=year,\n            month=month,\n            database=database,\n            schema=schema,\n            chunk_size=chunk_size,\n            force_reload=force_reload,\n            batch_run_id=batch_run_id,\n            batch_timestamp=batch_timestamp\n        )\n        \n        results['monthly_results'].append(month_result)\n        \n        if month_result['success']:\n            results['months_successful'] += 1\n            results['total_rows_loaded'] += month_result.get('rows_loaded', 0)\n        elif month_result.get('skipped'):\n            results['months_skipped'] += 1\n        elif month_result.get('gap'):\n            results['months_gap'] += 1\n        else:\n            results['months_failed'] += 1\n        \n        # Liberar memoria entre meses\n        gc.collect()\n    \n    print(f\"\\n{'=' * 80}\")\n    print(f\"RESUMEN: Exitosos={results['months_successful']}, Saltados={results['months_skipped']}, \"\n          f\"Brechas={results['months_gap']}, Fallidos={results['months_failed']}\")\n    print(f\"Total filas: {results['total_rows_loaded']:,}\")\n    print(f\"{'=' * 80}\")\n    \n    return results\n\n\ndef process_month_streaming(service, year, month, database, schema, chunk_size, force_reload, batch_run_id, batch_timestamp):\n    \"\"\"Procesa un mes con streaming - exporta cada chunk inmediatamente con columnas originales\"\"\"\n    run_id = str(uuid.uuid4())\n    \n    try:\n        # Verificar datos existentes\n        if not force_reload and service != 'taxi_zones':\n            existing_data = check_existing_data(database, schema, service, year, month)\n            if existing_data:\n                print(f\"    Saltando: {existing_data['count']:,} registros ya existen\")\n                return {\n                    'success': False,\n                    'skipped': True,\n                    'year': year,\n                    'month': month,\n                    'existing_count': existing_data['count']\n                }\n        \n        # Descargar archivo\n        if service == 'taxi_zones':\n            url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n            filename = \"taxi_zone_lookup.csv\"\n        else:\n            base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n            filename = f\"{service}_tripdata_{year:04d}-{month:02d}.parquet\"\n            url = f\"{base_url}/{filename}\"\n        \n        try:\n            response = requests.get(url, timeout=300)\n            response.raise_for_status()\n            df = pd.read_csv(url) if service == 'taxi_zones' else pd.read_parquet(url)\n            print(f\"    Descargado: {len(df):,} filas, {len(df.columns)} columnas\")\n        except Exception as e:\n            print(f\"    Brecha: archivo no existe\")\n            register_gap(database, schema, service, year, month)\n            return {'success': False, 'gap': True, 'year': year, 'month': month}\n        \n        total_rows = len(df)\n        total_chunks = (total_rows + chunk_size - 1) // chunk_size\n        \n        # Preparar conexi\u00f3n y tabla\n        conn = get_snowflake_connection(database.upper(), schema.upper())\n        table_name = get_table_name(service)\n        \n        # Crear tabla con columnas din\u00e1micas basadas en el esquema del Parquet\n        if service in ['yellow', 'green']:\n            ensure_table_exists_dynamic(conn, table_name, database, schema, df.columns, service)\n        elif service == 'taxi_zones':\n            ensure_table_exists_static(conn, table_name, database, schema, 'taxi_zones')\n        \n        # DELETE datos existentes del per\u00edodo (una sola vez)\n        if service != 'taxi_zones':\n            cursor = conn.cursor()\n            try:\n                existing = check_existing_data(database, schema, service, year, month)\n                if existing and existing['count'] > 0:\n                    print(f\"    Eliminando {existing['count']:,} registros existentes...\")\n                    cursor.execute(f\"\"\"\n                        DELETE FROM {database.upper()}.{schema.upper()}.{table_name.upper()}\n                        WHERE _data_year = {year} AND _data_month = {month}\n                    \"\"\")\n            finally:\n                cursor.close()\n        else:\n            # Taxi zones: truncar tabla\n            cursor = conn.cursor()\n            try:\n                cursor.execute(f\"TRUNCATE TABLE {database.upper()}.{schema.upper()}.{table_name.upper()}\")\n            finally:\n                cursor.close()\n        \n        print(f\"    Procesando y exportando en {total_chunks} chunks...\")\n        total_rows_inserted = 0\n        \n        # Procesar y exportar cada chunk INMEDIATAMENTE\n        for chunk_num in range(total_chunks):\n            start_idx = chunk_num * chunk_size\n            end_idx = min((chunk_num + 1) * chunk_size, total_rows)\n            \n            # Extraer solo el chunk necesario\n            chunk_df = df.iloc[start_idx:end_idx].copy()\n            \n            # Agregar metadatos de ingesta\n            chunk_df['_run_id'] = run_id\n            chunk_df['_batch_run_id'] = batch_run_id\n            chunk_df['_ingest_ts'] = batch_timestamp\n            chunk_df['_source_file'] = filename\n            chunk_df['_service_type'] = service\n            \n            if service != 'taxi_zones':\n                chunk_df['_data_year'] = year\n                chunk_df['_data_month'] = month\n            \n            # EXPORTAR INMEDIATAMENTE (sin transformaci\u00f3n a JSON)\n            success = export_chunk_streaming(\n                chunk_df, service, database, schema, table_name, conn\n            )\n            \n            if not success:\n                conn.close()\n                del df\n                gc.collect()\n                raise Exception(f\"Error exportando chunk {chunk_num + 1}\")\n            \n            total_rows_inserted += len(chunk_df)\n            \n            # Liberar memoria del chunk INMEDIATAMENTE\n            del chunk_df\n            \n            # Forzar garbage collection cada 5 chunks\n            if (chunk_num + 1) % 5 == 0:\n                gc.collect()\n            \n            if (chunk_num + 1) % 10 == 0 or (chunk_num + 1) == total_chunks:\n                print(f\"      Chunk {chunk_num + 1}/{total_chunks}: {total_rows_inserted:,} filas exportadas\")\n        \n        # Liberar DataFrame original INMEDIATAMENTE despu\u00e9s del loop\n        del df\n        gc.collect()\n        \n        conn.close()\n        \n        print(f\"    OK: {total_rows_inserted:,} filas\")\n        \n        if service in ['yellow', 'green']:\n            save_audit_coverage(database, schema, service, year, month)\n        \n        return {\n            'success': True,\n            'year': year,\n            'month': month,\n            'run_id': run_id,\n            'batch_run_id': batch_run_id,\n            'rows_loaded': total_rows_inserted\n        }\n            \n    except Exception as e:\n        print(f\"    Error: {e}\")\n        return {'success': False, 'year': year, 'month': month, 'error': str(e)}\n\n\ndef export_chunk_streaming(chunk_df, service, database, schema, table_name, conn):\n    \"\"\"Exporta un chunk individual inmediatamente - sin transformaci\u00f3n JSON\"\"\"\n    cursor = None\n    try:\n        # Convertir timestamps IN-PLACE para ahorrar memoria\n        for col in chunk_df.columns:\n            if pd.api.types.is_datetime64_any_dtype(chunk_df[col]):\n                chunk_df[col] = chunk_df[col].dt.strftime('%Y-%m-%d %H:%M:%S')\n            elif chunk_df[col].dtype == 'object' and len(chunk_df) > 0:\n                first_val = chunk_df[col].iloc[0]\n                if isinstance(first_val, datetime):\n                    chunk_df[col] = chunk_df[col].apply(\n                        lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if isinstance(x, datetime) else str(x)\n                    )\n        \n        if '_ingest_ts' in chunk_df.columns:\n            chunk_df['_ingest_ts'] = chunk_df['_ingest_ts'].astype(str)\n        \n        # Crear tabla temporal para este chunk\n        cursor = conn.cursor()\n        temp_table = f\"TMP_{uuid.uuid4().hex[:8]}\".upper()\n        \n        cursor.execute(f\"DROP TABLE IF EXISTS {database.upper()}.{schema.upper()}.{temp_table}\")\n        cursor.execute(f\"\"\"\n            CREATE TEMPORARY TABLE {temp_table} \n            LIKE {database.upper()}.{schema.upper()}.{table_name.upper()}\n        \"\"\")\n        \n        # Cargar chunk a tabla temporal\n        success, nchunks, nrows, _ = write_pandas(\n            conn=conn, df=chunk_df, table_name=temp_table,\n            database=database.upper(), schema=schema.upper(),\n            quote_identifiers=False\n        )\n        \n        if not success:\n            raise Exception(\"write_pandas fall\u00f3\")\n        \n        # INSERT directo desde temporal (APPEND)\n        cursor.execute(f\"\"\"\n            INSERT INTO {database.upper()}.{schema.upper()}.{table_name.upper()}\n            SELECT * FROM {database.upper()}.{schema.upper()}.{temp_table}\n        \"\"\")\n        \n        # Limpiar tabla temporal inmediatamente\n        cursor.execute(f\"DROP TABLE IF EXISTS {database.upper()}.{schema.upper()}.{temp_table}\")\n        cursor.close()\n        \n        return True\n        \n    except Exception as e:\n        print(f\"        Error en chunk export: {e}\")\n        if cursor:\n            cursor.close()\n        return False\n\n\ndef ensure_table_exists_dynamic(conn, table_name, database, schema, original_columns, service):\n    \"\"\"Crea tabla con todas las columnas originales del Parquet + metadatos\"\"\"\n    cursor = conn.cursor()\n    try:\n        cursor.execute(f\"\"\"\n            SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES\n            WHERE TABLE_SCHEMA = '{schema.upper()}' AND TABLE_NAME = '{table_name.upper()}'\n        \"\"\")\n        \n        if cursor.fetchone()[0] == 0:\n            # Mapeo de tipos comunes en Parquet a Snowflake\n            type_mapping = {\n                'int64': 'NUMBER',\n                'float64': 'FLOAT',\n                'object': 'VARCHAR',\n                'datetime64[ns]': 'TIMESTAMP',\n                'bool': 'BOOLEAN'\n            }\n            \n            # Construir definici\u00f3n de columnas\n            columns_def = []\n            for col in original_columns:\n                # Usar VARCHAR para todas las columnas originales (Snowflake infiere bien)\n                columns_def.append(f\"{col} VARCHAR\")\n            \n            # Agregar columnas de metadatos\n            metadata_cols = [\n                \"_run_id VARCHAR\",\n                \"_batch_run_id VARCHAR\",\n                \"_ingest_ts TIMESTAMP\",\n                \"_source_file VARCHAR\",\n                \"_service_type VARCHAR\",\n                \"_data_year NUMBER\",\n                \"_data_month NUMBER\"\n            ]\n            \n            all_columns = columns_def + metadata_cols\n            columns_and_types = \",\\n                \".join(all_columns)\n            \n            cursor.execute(f\"\"\"\n                CREATE TABLE {database.upper()}.{schema.upper()}.{table_name.upper()} (\n                    {columns_and_types}\n                )\n            \"\"\")\n            print(f\"    Tabla {table_name} creada con {len(original_columns)} columnas + 7 metadatos\")\n    finally:\n        cursor.close()\n\n\ndef ensure_table_exists_static(conn, table_name, database, schema, table_type):\n    \"\"\"Crea tabla est\u00e1tica para taxi_zones\"\"\"\n    cursor = conn.cursor()\n    try:\n        cursor.execute(f\"\"\"\n            SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES\n            WHERE TABLE_SCHEMA = '{schema.upper()}' AND TABLE_NAME = '{table_name.upper()}'\n        \"\"\")\n        \n        if cursor.fetchone()[0] == 0:\n            if table_type == 'taxi_zones':\n                columns_and_types = \"\"\"\n                    LocationID NUMBER,\n                    Borough VARCHAR,\n                    Zone VARCHAR,\n                    service_zone VARCHAR,\n                    _run_id VARCHAR,\n                    _batch_run_id VARCHAR,\n                    _ingest_ts TIMESTAMP,\n                    _source_file VARCHAR,\n                    _service_type VARCHAR\n                \"\"\"\n            \n            cursor.execute(f\"\"\"\n                CREATE TABLE {database.upper()}.{schema.upper()}.{table_name.upper()} (\n                    {columns_and_types}\n                )\n            \"\"\")\n    finally:\n        cursor.close()\n\n\ndef get_table_name(service):\n    return 'TAXI_ZONES' if service == 'taxi_zones' else f\"{service}_tripdata\".upper()\n\n\ndef get_snowflake_connection(database, schema):\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_loader = ConfigFileLoader(config_path, 'default')\n    config = config_loader.config\n    \n    return snowflake.connector.connect(\n        account=config.get('SNOWFLAKE_ACCOUNT'),\n        user=config.get('SNOWFLAKE_USER'),\n        password=config.get('SNOWFLAKE_PASSWORD'),\n        warehouse=config.get('SNOWFLAKE_WAREHOUSE'),\n        database=database,\n        schema=schema,\n        insecure_mode=True\n    )\n\n\ndef check_existing_data(database, schema, service, year, month):\n    try:\n        conn = get_snowflake_connection(database, schema)\n        table_name = get_table_name(service)\n        cursor = conn.cursor()\n        \n        cursor.execute(f\"\"\"\n            SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES\n            WHERE TABLE_SCHEMA = '{schema.upper()}' AND TABLE_NAME = '{table_name.upper()}'\n        \"\"\")\n        \n        if cursor.fetchone()[0] == 0:\n            cursor.close()\n            conn.close()\n            return None\n        \n        cursor.execute(f\"\"\"\n            SELECT COUNT(*) as cnt\n            FROM {database}.{schema}.{table_name}\n            WHERE _data_year = {year} AND _data_month = {month}\n        \"\"\")\n        \n        count = cursor.fetchone()[0]\n        cursor.close()\n        conn.close()\n        \n        return {'count': int(count)} if count > 0 else None\n    except:\n        return None\n\n\ndef ensure_audit_table_exists(conn, database, schema):\n    cursor = conn.cursor()\n    try:\n        cursor.execute(f\"\"\"\n            SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES\n            WHERE TABLE_SCHEMA = '{schema.upper()}' AND TABLE_NAME = 'AUDIT_COVERAGE'\n        \"\"\")\n        \n        if cursor.fetchone()[0] == 0:\n            cursor.execute(f\"\"\"\n                CREATE TABLE {database.upper()}.{schema.upper()}.AUDIT_COVERAGE (\n                    service_type VARCHAR,\n                    _data_year NUMBER,\n                    _data_month NUMBER,\n                    row_count NUMBER,\n                    gap BOOLEAN,\n                    registered_at TIMESTAMP\n                )\n            \"\"\")\n    finally:\n        cursor.close()\n\n\ndef register_gap(database, schema, service, year, month):\n    try:\n        gap_df = pd.DataFrame([{\n            'service_type': service,\n            '_data_year': year if service != 'taxi_zones' else None,\n            '_data_month': month if service != 'taxi_zones' else None,\n            'row_count': 0,\n            'gap': True,\n            'registered_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        }])\n        \n        conn = get_snowflake_connection(database, schema)\n        try:\n            ensure_audit_table_exists(conn, database, schema)\n            write_pandas(conn=conn, df=gap_df, table_name='AUDIT_COVERAGE',\n                        database=database.upper(), schema=schema.upper(),\n                        auto_create_table=False, quote_identifiers=False)\n        finally:\n            conn.close()\n    except:\n        pass\n\n\ndef save_audit_coverage(database, schema, service, year, month):\n    try:\n        conn = get_snowflake_connection(database, schema)\n        ensure_audit_table_exists(conn, database, schema)\n        \n        query = f\"\"\"\n        SELECT {year} as _data_year, {month} as _data_month,\n               COUNT(*) as row_count, '{service}' as service_type,\n               FALSE as gap, CURRENT_TIMESTAMP() as registered_at\n        FROM {database}.{schema}.{get_table_name(service)}\n        WHERE _data_year = {year} AND _data_month = {month}\n        \"\"\"\n        \n        result = pd.read_sql(query, conn)\n        \n        if len(result) > 0:\n            write_pandas(conn=conn, df=result, table_name='AUDIT_COVERAGE',\n                        database=database.upper(), schema=schema.upper(),\n                        auto_create_table=False, quote_identifiers=False)\n        \n        conn.close()\n    except:\n        pass", "file_path": "data_loaders/ingest_data.py", "language": "python", "type": "data_loader", "uuid": "ingest_data"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/uplifting_shadow.py:data_loader:python:uplifting shadow": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your data loading logic here\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/uplifting_shadow.py", "language": "python", "type": "data_loader", "uuid": "uplifting_shadow"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/backfill_nyc_taxi_data/metadata.yaml:pipeline:yaml:backfill nyc taxi data/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/ingest_data.py\n    file_source:\n      path: data_loaders/ingest_data.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_data\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_data\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt/demo\n    file_path: dbt/demo/models/staging/stg_yellow.sql\n    file_source:\n      path: dbt/demo/models/staging/stg_yellow.sql\n      project_path: dbt/demo\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/demo/models/staging/stg_yellow\n  retry_config: null\n  status: failed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/demo/models/staging/stg_yellow\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-25 16:30:12.578109+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: backfill_nyc_taxi_data\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: backfill_nyc_taxi_data\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/backfill_nyc_taxi_data/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "backfill_nyc_taxi_data/metadata"}, "pipelines/backfill_nyc_taxi_data/__init__.py:pipeline:python:backfill nyc taxi data/  init  ": {"content": "", "file_path": "pipelines/backfill_nyc_taxi_data/__init__.py", "language": "python", "type": "pipeline", "uuid": "backfill_nyc_taxi_data/__init__"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/taxi_modelling/metadata.yaml:pipeline:yaml:taxi modelling/metadata": {"content": "created_at: '2025-10-05 16:59:11.874501+00:00'\ndescription: null\nname: taxi_modelling\ntags: []\ntype: python\nuuid: taxi_modelling\n", "file_path": "pipelines/taxi_modelling/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "taxi_modelling/metadata"}, "pipelines/taxi_modelling/__init__.py:pipeline:python:taxi modelling/  init  ": {"content": "", "file_path": "pipelines/taxi_modelling/__init__.py", "language": "python", "type": "pipeline", "uuid": "taxi_modelling/__init__"}, "/home/src/scheduler/data_loaders/ingest_data.py:data_loader:python:home/src/scheduler/data loaders/ingest data": {"content": "import pandas as pd\nimport requests\nfrom datetime import datetime\nimport uuid\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.snowflake import Snowflake\nfrom os import path\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport gc\nimport time\n\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n# Configuraci\u00f3n de reintentos\nMAX_RETRIES = 3\nRETRY_DELAY = 5  # segundos\nBACKOFF_MULTIPLIER = 2  # delay exponencial\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Pipeline de backfill con streaming y reintentos\n    Capa Bronze: mantiene columnas originales + metadatos de ingesta\n    \n    Par\u00e1metros:\n    - service: 'yellow', 'green', 'taxi_zones', etc.\n    - year: A\u00f1o a procesar\n    - months: Lista de meses [1,2,3] o None para todos\n    - chunk_size: Filas por chunk (default: 1000000)\n    - force_reload: Sobrescribir datos existentes (default: False)\n    - max_retries: N\u00famero m\u00e1ximo de reintentos (default: 3)\n    \"\"\"\n\n    print(f\"DEBUG kwargs completos: {kwargs}\")\n    print(f\"DEBUG year en kwargs: {'year' in kwargs}\")\n    print(f\"DEBUG valor de year: {kwargs.get('year', 'NO ENCONTRADO')}\")\n    \n    service = kwargs.get('service', 'yellow')\n    year = int(kwargs.get('year', 2015))\n    months = kwargs.get('months', None)\n    database = get_secret_value('SNOWFLAKE_DATABASE')\n    schema = get_secret_value('SNOWFLAKE_SCHEMA')\n    chunk_size = int(kwargs.get('chunk_size', 1000000))\n    force_reload = kwargs.get('force_reload', False)\n    max_retries = int(kwargs.get('max_retries', MAX_RETRIES))\n    \n    # Normalizar months a lista de enteros\n    if months is None:\n        months = list(range(1, 13))\n    elif isinstance(months, int):\n        months = [months]\n    elif isinstance(months, str):\n        months = [int(m.strip()) for m in months.strip('[]').split(',')]\n    elif isinstance(months, list):\n        months = [int(m) for m in months]\n    \n    batch_run_id = str(uuid.uuid4())\n    \n    execution_date = kwargs.get('execution_date')\n    if execution_date:\n        if isinstance(execution_date, datetime):\n            batch_timestamp = execution_date.strftime('%Y-%m-%d %H:%M:%S')\n        else:\n            batch_timestamp = str(execution_date)\n    else:\n        batch_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \n    print(f\"=\" * 80)\n    print(f\"BACKFILL STREAMING CON REINTENTOS - {service.upper()} {year}\")\n    print(f\"Meses: {months}, Chunk size: {chunk_size:,}, Max reintentos: {max_retries}\")\n    print(f\"Batch ID: {batch_run_id[:8]}\")\n    print(f\"=\" * 80)\n    \n    results = {\n        'batch_run_id': batch_run_id,\n        'service': service,\n        'year': year,\n        'batch_timestamp': batch_timestamp,\n        'months_attempted': len(months),\n        'months_successful': 0,\n        'months_skipped': 0,\n        'months_failed': 0,\n        'months_gap': 0,\n        'total_rows_loaded': 0,\n        'monthly_results': []\n    }\n    \n    for month in months:\n        print(f\"\\n[{month:02d}] Procesando {service} {year}-{month:02d}\")\n        \n        month_result = process_month_streaming(\n            service=service,\n            year=year,\n            month=month,\n            database=database,\n            schema=schema,\n            chunk_size=chunk_size,\n            force_reload=force_reload,\n            batch_run_id=batch_run_id,\n            batch_timestamp=batch_timestamp,\n            max_retries=max_retries\n        )\n        \n        results['monthly_results'].append(month_result)\n        \n        if month_result['success']:\n            results['months_successful'] += 1\n            results['total_rows_loaded'] += month_result.get('rows_loaded', 0)\n        elif month_result.get('skipped'):\n            results['months_skipped'] += 1\n        elif month_result.get('gap'):\n            results['months_gap'] += 1\n        else:\n            results['months_failed'] += 1\n        \n        gc.collect()\n    \n    print(f\"\\n{'=' * 80}\")\n    print(f\"RESUMEN: Exitosos={results['months_successful']}, Saltados={results['months_skipped']}, \"\n          f\"Brechas={results['months_gap']}, Fallidos={results['months_failed']}\")\n    print(f\"Total filas: {results['total_rows_loaded']:,}\")\n    print(f\"{'=' * 80}\")\n    \n    return results\n\n\ndef retry_with_backoff(func, *args, max_retries=MAX_RETRIES, retry_delay=RETRY_DELAY, **kwargs):\n    \"\"\"Ejecuta una funci\u00f3n con reintentos exponenciales\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            \n            wait_time = retry_delay * (BACKOFF_MULTIPLIER ** attempt)\n            print(f\"    Intento {attempt + 1}/{max_retries} fall\u00f3: {e}\")\n            print(f\"    Reintentando en {wait_time}s...\")\n            time.sleep(wait_time)\n    \n    raise Exception(f\"Fall\u00f3 despu\u00e9s de {max_retries} intentos\")\n\n\ndef download_file_with_retry(url, service, max_retries=MAX_RETRIES):\n    \"\"\"Descarga archivo con reintentos\"\"\"\n    def _download():\n        response = requests.get(url, timeout=300)\n        response.raise_for_status()\n        \n        if service == 'taxi_zones':\n            return pd.read_csv(url)\n        else:\n            return pd.read_parquet(url)\n    \n    return retry_with_backoff(_download, max_retries=max_retries)\n\n\ndef process_month_streaming(service, year, month, database, schema, chunk_size, \n                           force_reload, batch_run_id, batch_timestamp, max_retries):\n    \"\"\"Procesa un mes con streaming y reintentos\"\"\"\n    run_id = str(uuid.uuid4())\n    \n    try:\n        # Verificar datos existentes\n        if not force_reload and service != 'taxi_zones':\n            existing_data = check_existing_data(database, schema, service, year, month)\n            if existing_data:\n                print(f\"    Saltando: {existing_data['count']:,} registros ya existen\")\n                return {\n                    'success': False,\n                    'skipped': True,\n                    'year': year,\n                    'month': month,\n                    'existing_count': existing_data['count']\n                }\n        \n        # Descargar archivo con reintentos\n        if service == 'taxi_zones':\n            url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n            filename = \"taxi_zone_lookup.csv\"\n        else:\n            base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n            filename = f\"{service}_tripdata_{year:04d}-{month:02d}.parquet\"\n            url = f\"{base_url}/{filename}\"\n        \n        try:\n            df = download_file_with_retry(url, service, max_retries=max_retries)\n            print(f\"    Descargado: {len(df):,} filas, {len(df.columns)} columnas\")\n        except Exception as e:\n            print(f\"    Brecha: archivo no existe despu\u00e9s de {max_retries} intentos\")\n            register_gap(database, schema, service, year, month)\n            return {'success': False, 'gap': True, 'year': year, 'month': month}\n        \n        total_rows = len(df)\n        total_chunks = (total_rows + chunk_size - 1) // chunk_size\n        \n        # Preparar conexi\u00f3n y tabla con reintentos\n        conn = retry_with_backoff(\n            get_snowflake_connection, \n            database.upper(), \n            schema.upper(),\n            max_retries=max_retries\n        )\n        \n        table_name = get_table_name(service)\n        \n        # Crear tabla con columnas din\u00e1micas\n        if service in ['yellow', 'green']:\n            ensure_table_exists_dynamic(conn, table_name, database, schema, df.columns, service)\n        elif service == 'taxi_zones':\n            ensure_table_exists_static(conn, table_name, database, schema, 'taxi_zones')\n        \n        # DELETE datos existentes del per\u00edodo\n        if service != 'taxi_zones':\n            cursor = conn.cursor()\n            try:\n                existing = check_existing_data(database, schema, service, year, month)\n                if existing and existing['count'] > 0:\n                    print(f\"    Eliminando {existing['count']:,} registros existentes...\")\n                    retry_with_backoff(\n                        cursor.execute,\n                        f\"\"\"DELETE FROM {database.upper()}.{schema.upper()}.{table_name.upper()}\n                            WHERE _data_year = {year} AND _data_month = {month}\"\"\",\n                        max_retries=max_retries\n                    )\n            finally:\n                cursor.close()\n        else:\n            cursor = conn.cursor()\n            try:\n                retry_with_backoff(\n                    cursor.execute,\n                    f\"TRUNCATE TABLE {database.upper()}.{schema.upper()}.{table_name.upper()}\",\n                    max_retries=max_retries\n                )\n            finally:\n                cursor.close()\n        \n        print(f\"    Procesando y exportando en {total_chunks} chunks...\")\n        total_rows_inserted = 0\n        \n        # Procesar y exportar cada chunk con reintentos\n        for chunk_num in range(total_chunks):\n            start_idx = chunk_num * chunk_size\n            end_idx = min((chunk_num + 1) * chunk_size, total_rows)\n            \n            chunk_df = df.iloc[start_idx:end_idx].copy()\n            \n            # Agregar metadatos\n            chunk_df['_run_id'] = run_id\n            chunk_df['_batch_run_id'] = batch_run_id\n            chunk_df['_ingest_ts'] = batch_timestamp\n            chunk_df['_source_file'] = filename\n            chunk_df['_service_type'] = service\n            \n            if service != 'taxi_zones':\n                chunk_df['_data_year'] = year\n                chunk_df['_data_month'] = month\n            \n            # EXPORTAR con reintentos\n            success = retry_with_backoff(\n                export_chunk_streaming,\n                chunk_df, service, database, schema, table_name, conn,\n                max_retries=max_retries\n            )\n            \n            if not success:\n                conn.close()\n                del df\n                gc.collect()\n                raise Exception(f\"Error exportando chunk {chunk_num + 1}\")\n            \n            total_rows_inserted += len(chunk_df)\n            del chunk_df\n            \n            if (chunk_num + 1) % 5 == 0:\n                gc.collect()\n            \n            if (chunk_num + 1) % 10 == 0 or (chunk_num + 1) == total_chunks:\n                print(f\"      Chunk {chunk_num + 1}/{total_chunks}: {total_rows_inserted:,} filas exportadas\")\n        \n        del df\n        gc.collect()\n        conn.close()\n        \n        print(f\"    OK: {total_rows_inserted:,} filas\")\n        \n        if service in ['yellow', 'green']:\n            save_audit_coverage(database, schema, service, year, month)\n        \n        return {\n            'success': True,\n            'year': year,\n            'month': month,\n            'run_id': run_id,\n            'batch_run_id': batch_run_id,\n            'rows_loaded': total_rows_inserted\n        }\n            \n    except Exception as e:\n        print(f\"    Error final: {e}\")\n        return {'success': False, 'year': year, 'month': month, 'error': str(e)}\n\n\ndef export_chunk_streaming(chunk_df, service, database, schema, table_name, conn):\n    \"\"\"Exporta un chunk individual - lanza excepci\u00f3n para reintentos\"\"\"\n    cursor = None\n    try:\n        # Convertir timestamps\n        for col in chunk_df.columns:\n            if pd.api.types.is_datetime64_any_dtype(chunk_df[col]):\n                chunk_df[col] = chunk_df[col].dt.strftime('%Y-%m-%d %H:%M:%S')\n            elif chunk_df[col].dtype == 'object' and len(chunk_df) > 0:\n                first_val = chunk_df[col].iloc[0]\n                if isinstance(first_val, datetime):\n                    chunk_df[col] = chunk_df[col].apply(\n                        lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if isinstance(x, datetime) else str(x)\n                    )\n        \n        if '_ingest_ts' in chunk_df.columns:\n            chunk_df['_ingest_ts'] = chunk_df['_ingest_ts'].astype(str)\n        \n        cursor = conn.cursor()\n        temp_table = f\"TMP_{uuid.uuid4().hex[:8]}\".upper()\n        \n        cursor.execute(f\"DROP TABLE IF EXISTS {database.upper()}.{schema.upper()}.{temp_table}\")\n        cursor.execute(f\"\"\"\n            CREATE TEMPORARY TABLE {temp_table} \n            LIKE {database.upper()}.{schema.upper()}.{table_name.upper()}\n        \"\"\")\n        \n        success, nchunks, nrows, _ = write_pandas(\n            conn=conn, df=chunk_df, table_name=temp_table,\n            database=database.upper(), schema=schema.upper(),\n            quote_identifiers=False\n        )\n        \n        if not success:\n            raise Exception(\"write_pandas fall\u00f3\")\n        \n        cursor.execute(f\"\"\"\n            INSERT INTO {database.upper()}.{schema.upper()}.{table_name.upper()}\n            SELECT * FROM {database.upper()}.{schema.upper()}.{temp_table}\n        \"\"\")\n        \n        cursor.execute(f\"DROP TABLE IF EXISTS {database.upper()}.{schema.upper()}.{temp_table}\")\n        cursor.close()\n        \n        return True\n        \n    except Exception as e:\n        if cursor:\n            cursor.close()\n        raise Exception(f\"Error en chunk export: {e}\")\n\n\ndef ensure_table_exists_dynamic(conn, table_name, database, schema, original_columns, service):\n    \"\"\"Crea tabla con todas las columnas originales del Parquet + metadatos\"\"\"\n    cursor = conn.cursor()\n    try:\n        cursor.execute(f\"\"\"\n            SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES\n            WHERE TABLE_SCHEMA = '{schema.upper()}' AND TABLE_NAME = '{table_name.upper()}'\n        \"\"\")\n        \n        if cursor.fetchone()[0] == 0:\n            columns_def = []\n            for col in original_columns:\n                columns_def.append(f\"{col} VARCHAR\")\n            \n            metadata_cols = [\n                \"_run_id VARCHAR\",\n                \"_batch_run_id VARCHAR\",\n                \"_ingest_ts TIMESTAMP\",\n                \"_source_file VARCHAR\",\n                \"_service_type VARCHAR\",\n                \"_data_year NUMBER\",\n                \"_data_month NUMBER\"\n            ]\n            \n            all_columns = columns_def + metadata_cols\n            columns_and_types = \",\\n                \".join(all_columns)\n            \n            cursor.execute(f\"\"\"\n                CREATE TABLE {database.upper()}.{schema.upper()}.{table_name.upper()} (\n                    {columns_and_types}\n                )\n            \"\"\")\n            print(f\"    Tabla {table_name} creada con {len(original_columns)} columnas + 7 metadatos\")\n    finally:\n        cursor.close()\n\n\ndef ensure_table_exists_static(conn, table_name, database, schema, table_type):\n    \"\"\"Crea tabla est\u00e1tica para taxi_zones\"\"\"\n    cursor = conn.cursor()\n    try:\n        cursor.execute(f\"\"\"\n            SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES\n            WHERE TABLE_SCHEMA = '{schema.upper()}' AND TABLE_NAME = '{table_name.upper()}'\n        \"\"\")\n        \n        if cursor.fetchone()[0] == 0:\n            if table_type == 'taxi_zones':\n                columns_and_types = \"\"\"\n                    LocationID NUMBER,\n                    Borough VARCHAR,\n                    Zone VARCHAR,\n                    service_zone VARCHAR,\n                    _run_id VARCHAR,\n                    _batch_run_id VARCHAR,\n                    _ingest_ts TIMESTAMP,\n                    _source_file VARCHAR,\n                    _service_type VARCHAR\n                \"\"\"\n            \n            cursor.execute(f\"\"\"\n                CREATE TABLE {database.upper()}.{schema.upper()}.{table_name.upper()} (\n                    {columns_and_types}\n                )\n            \"\"\")\n    finally:\n        cursor.close()\n\n\ndef get_table_name(service):\n    return 'TAXI_ZONES' if service == 'taxi_zones' else f\"{service}_tripdata\".upper()\n\n\ndef get_snowflake_connection(database, schema):\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_loader = ConfigFileLoader(config_path, 'default')\n    config = config_loader.config\n    \n    return snowflake.connector.connect(\n        account=config.get('SNOWFLAKE_ACCOUNT'),\n        user=config.get('SNOWFLAKE_USER'),\n        password=config.get('SNOWFLAKE_PASSWORD'),\n        warehouse=config.get('SNOWFLAKE_WAREHOUSE'),\n        database=database,\n        schema=schema,\n        insecure_mode=True\n    )\n\n\ndef check_existing_data(database, schema, service, year, month):\n    try:\n        conn = get_snowflake_connection(database, schema)\n        table_name = get_table_name(service)\n        cursor = conn.cursor()\n        \n        cursor.execute(f\"\"\"\n            SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES\n            WHERE TABLE_SCHEMA = '{schema.upper()}' AND TABLE_NAME = '{table_name.upper()}'\n        \"\"\")\n        \n        if cursor.fetchone()[0] == 0:\n            cursor.close()\n            conn.close()\n            return None\n        \n        cursor.execute(f\"\"\"\n            SELECT COUNT(*) as cnt\n            FROM {database}.{schema}.{table_name}\n            WHERE _data_year = {year} AND _data_month = {month}\n        \"\"\")\n        \n        count = cursor.fetchone()[0]\n        cursor.close()\n        conn.close()\n        \n        return {'count': int(count)} if count > 0 else None\n    except:\n        return None\n\n\ndef ensure_audit_table_exists(conn, database, schema):\n    cursor = conn.cursor()\n    try:\n        cursor.execute(f\"\"\"\n            SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES\n            WHERE TABLE_SCHEMA = '{schema.upper()}' AND TABLE_NAME = 'AUDIT_COVERAGE'\n        \"\"\")\n        \n        if cursor.fetchone()[0] == 0:\n            cursor.execute(f\"\"\"\n                CREATE TABLE {database.upper()}.{schema.upper()}.AUDIT_COVERAGE (\n                    service_type VARCHAR,\n                    _data_year NUMBER,\n                    _data_month NUMBER,\n                    row_count NUMBER,\n                    gap BOOLEAN,\n                    registered_at TIMESTAMP\n                )\n            \"\"\")\n    finally:\n        cursor.close()\n\n\ndef register_gap(database, schema, service, year, month):\n    try:\n        gap_df = pd.DataFrame([{\n            'service_type': service,\n            '_data_year': year if service != 'taxi_zones' else None,\n            '_data_month': month if service != 'taxi_zones' else None,\n            'row_count': 0,\n            'gap': True,\n            'registered_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        }])\n        \n        conn = get_snowflake_connection(database, schema)\n        try:\n            ensure_audit_table_exists(conn, database, schema)\n            write_pandas(conn=conn, df=gap_df, table_name='AUDIT_COVERAGE',\n                        database=database.upper(), schema=schema.upper(),\n                        auto_create_table=False, quote_identifiers=False)\n        finally:\n            conn.close()\n    except:\n        pass\n\n\ndef save_audit_coverage(database, schema, service, year, month):\n    try:\n        conn = get_snowflake_connection(database, schema)\n        ensure_audit_table_exists(conn, database, schema)\n        \n        query = f\"\"\"\n        SELECT {year} as _data_year, {month} as _data_month,\n               COUNT(*) as row_count, '{service}' as service_type,\n               FALSE as gap, CURRENT_TIMESTAMP() as registered_at\n        FROM {database}.{schema}.{get_table_name(service)}\n        WHERE _data_year = {year} AND _data_month = {month}\n        \"\"\"\n        \n        result = pd.read_sql(query, conn)\n        \n        if len(result) > 0:\n            write_pandas(conn=conn, df=result, table_name='AUDIT_COVERAGE',\n                        database=database.upper(), schema=schema.upper(),\n                        auto_create_table=False, quote_identifiers=False)\n        \n        conn.close()\n    except:\n        pass", "file_path": "/home/src/scheduler/data_loaders/ingest_data.py", "language": "python", "type": "data_loader", "uuid": "ingest_data"}, "/home/src/scheduler/dbt/demo/models/staging/stg_enriched.sql:dbt:sql:home/src/scheduler/dbt/demo/models/staging/stg enriched": {"content": "{{ config(\n    materialized='table'\n) }}\n\nwith trips as (\n    select *\n    from {{ ref('stg_trips') }} \n),\nzones as (\n    select *\n    from {{ source('bronze', 'taxi_zones') }}\n)\n\nselect\n    t.*,\n    DATEDIFF(second, t.pickup_datetime, t.dropoff_datetime) AS trip_duration_seconds,\n    zp.zone as pickup_zone,\n    zp.borough as pickup_borough,\n    zd.zone as dropoff_zone,\n    zd.borough as dropoff_borough\nfrom trips t\nleft join zones zp on t.pu_location_id = zp.zone\nleft join zones zd on t.do_location_id = zd.borough", "file_path": "/home/src/scheduler/dbt/demo/models/staging/stg_enriched.sql", "language": "sql", "type": "dbt", "uuid": "dbt/demo/models/staging/stg_enriched"}, "/home/src/scheduler/dbt/demo/models/staging/stg_trips.sql:dbt:sql:home/src/scheduler/dbt/demo/models/staging/stg trips": {"content": "{{ config(\n    materialized='table'\n) }}\n\nwith yellow as (\n    select *, 'yellow' as service_type\n    from {{ ref('stg_yellow') }}\n),\ngreen as (\n    select *, 'green' as service_type\n    from {{ ref('stg_green') }}\n)\n\nselect *\nfrom yellow\nunion all\nselect *\nfrom green", "file_path": "/home/src/scheduler/dbt/demo/models/staging/stg_trips.sql", "language": "sql", "type": "dbt", "uuid": "dbt/demo/models/staging/stg_trips"}, "/home/src/scheduler/dbt/demo/models/marts/dim_payment_type.sql:dbt:sql:home/src/scheduler/dbt/demo/models/marts/dim payment type": {"content": "{{ config(\n    materialized='table'\n) }}\n\nselect distinct\n    {{ dbt_utils.generate_surrogate_key(['payment_type']) }} as payment_type_sk,\n    payment_type\nfrom {{ ref('stg_enriched') }}\nwhere payment_type is not null", "file_path": "/home/src/scheduler/dbt/demo/models/marts/dim_payment_type.sql", "language": "sql", "type": "dbt", "uuid": "dbt/demo/models/marts/dim_payment_type"}, "/home/src/scheduler/dbt/demo/models/marts/dim_rate_code.sql:dbt:sql:home/src/scheduler/dbt/demo/models/marts/dim rate code": {"content": "{{ config(\n    materialized='table'\n) }}\n\nselect distinct\n    cast(rate_code_id as int) as rate_code_sk, \n    case rate_code_id\n        when 1 then 'Standard rate'\n        when 2 then 'JFK'\n        when 3 then 'Newark'\n        when 4 then 'Nassau or Westchester'\n        when 5 then 'Negotiated fare'\n        when 6 then 'Group ride'\n        when 99 then 'Null/unknown'\n        else 'Other'\n    end as rate_code_name\nfrom {{ ref('stg_enriched') }}\nwhere rate_code_id is not null\n", "file_path": "/home/src/scheduler/dbt/demo/models/marts/dim_rate_code.sql", "language": "sql", "type": "dbt", "uuid": "dbt/demo/models/marts/dim_rate_code"}, "/home/src/scheduler/dbt/demo/models/marts/dim_service_type.sql:dbt:sql:home/src/scheduler/dbt/demo/models/marts/dim service type": {"content": "{{ config(\n    materialized='table'\n) }}\n\nselect distinct\n    {{ dbt_utils.generate_surrogate_key(['service_type']) }} as service_type_sk,\n    service_type as service_type_name\nfrom {{ ref('stg_enriched') }}\nwhere service_type is not null", "file_path": "/home/src/scheduler/dbt/demo/models/marts/dim_service_type.sql", "language": "sql", "type": "dbt", "uuid": "dbt/demo/models/marts/dim_service_type"}, "/home/src/scheduler/dbt/demo/models/marts/dim_vendor.sql:dbt:sql:home/src/scheduler/dbt/demo/models/marts/dim vendor": {"content": "{{ config(\n    materialized='table'\n) }}\n\n\nselect distinct\n    cast(vendor_id as int) as vendor_sk, \n    case vendor_id\n        when 1 then 'Creative Mobile Technologies, LLC' \n        when 2 then 'Curb Mobility, LLC' \n        when 6 then 'Myle Technologies Inc' \n        when 7 then 'Helix' \n        else 'Unknown'\n    end as vendor_name\nfrom {{ ref('stg_enriched') }}\nwhere vendor_id is not null", "file_path": "/home/src/scheduler/dbt/demo/models/marts/dim_vendor.sql", "language": "sql", "type": "dbt", "uuid": "dbt/demo/models/marts/dim_vendor"}, "/home/src/scheduler/dbt/demo/models/marts/dim_zone.sql:dbt:sql:home/src/scheduler/dbt/demo/models/marts/dim zone": {"content": "{{ config(\n    materialized='table'\n) }}\n\nselect distinct\n    -- La LocationID es la clave natural y subrogada\n    cast(locationid as int) as zone_sk, \n    Borough,\n    Zone,\n    service_zone\nfrom {{ source('bronze', 'taxi_zones') }}\nwhere locationid is not null", "file_path": "/home/src/scheduler/dbt/demo/models/marts/dim_zone.sql", "language": "sql", "type": "dbt", "uuid": "dbt/demo/models/marts/dim_zone"}, "/home/src/scheduler/dbt/demo/models/marts/fct_trips.sql:dbt:sql:home/src/scheduler/dbt/demo/models/marts/fct trips": {"content": "{{ config(\n    materialized='table',\n    cluster_by=['pickup_date_sk', 'pu_zone_sk'] \n) }}\n\nwith trips as (\n    select *\n    from {{ ref('stg_enriched') }} -- Consume el modelo final de Silver\n),\n\nfinal_fact as (\n    select\n        -- CLAVE PRIMARIA DEL HECHO (Hash del registro)\n        {{ dbt_utils.generate_surrogate_key([\n            'pickup_datetime', 'dropoff_datetime', 'pu_location_id', \n            'do_location_id', 'total_amount', 'service_type'\n        ]) }} as trip_id, \n\n        -- CLAVES SUBROGADAS (SK)\n        -- Claves de Fecha (Unir con dim_date)\n        D_PICKUP.date_sk as pickup_date_sk,\n        D_DROPOFF.date_sk as dropoff_date_sk,\n        \n        -- Claves de Zona (LocationID)\n        T1.pu_location_id as pu_zone_sk, \n        T1.do_location_id as do_zone_sk,\n        \n        -- Otras Claves de Dimensi\u00f3n\n        PT.payment_type_sk,\n        V.vendor_sk,\n        RC.rate_code_sk,\n        ST.service_type_sk,\n\n        -- M\u00c9TRICAS (Medidas)\n        T1.trip_distance,\n        T1.trip_duration_seconds,\n        T1.fare_amount,\n        T1.total_amount,\n        T1.tip_amount,\n        T1.extra,\n        T1.mta_tax,\n        T1.tolls_amount,\n        T1.congestion_surcharge,\n        T1.airport_fee,\n        T1.cbd_congestion_fee,\n\n        -- Metadatos para auditor\u00eda\n        T1.service_type\n        \n    from trips T1\n    \n    -- UNIONES A DIMENSIONES\n    left join {{ ref('dim_date') }} D_PICKUP \n        on date_trunc('day', T1.pickup_datetime)::date = D_PICKUP.full_date\n    left join {{ ref('dim_date') }} D_DROPOFF \n        on date_trunc('day', T1.dropoff_datetime)::date = D_DROPOFF.full_date\n        \n    left join {{ ref('dim_payment_type') }} PT on T1.payment_type = PT.payment_type\n    left join {{ ref('dim_vendor') }} V on T1.vendor_id = V.vendor_sk\n    left join {{ ref('dim_rate_code') }} RC on T1.rate_code_id = RC.rate_code_sk\n    left join {{ ref('dim_service_type') }} ST on T1.service_type = ST.service_type_name\n)\n\nselect * from final_fact", "file_path": "/home/src/scheduler/dbt/demo/models/marts/fct_trips.sql", "language": "sql", "type": "dbt", "uuid": "dbt/demo/models/marts/fct_trips"}, "/home/src/scheduler/dbt/demo/models/staging/stg_yellow.sql:dbt:sql:home/src/scheduler/dbt/demo/models/staging/stg yellow": {"content": "-- Modelo Silver: Yellow Taxi Trip, limpieza y estandarizaci\u00f3n\n{{ config(materialized='table') }}\n\nwith raw as (\n    select *\n    from {{ source('bronze', 'yellow_tripdata') }}\n),\n\nstandardized as (\n    select\n        -- Convertir fechas a timestamp y estandarizar zona horaria NYC\n        convert_timezone('America/New_York', TPEP_PICKUP_DATETIME) as pickup_datetime,\n        convert_timezone('America/New_York', TPEP_DROPOFF_DATETIME) as dropoff_datetime,\n        -- Convertir distancias y tarifas a float\n        cast(TRIP_DISTANCE as float) as trip_distance,\n        cast(FARE_AMOUNT as float) as fare_amount,\n        cast(TOTAL_AMOUNT as float) as total_amount,\n        cast(TIP_AMOUNT as float) as tip_amount,\n        cast(EXTRA as float) as extra,\n        cast(MTA_TAX as float) as mta_tax,\n        cast(TOLLS_AMOUNT as float) as tolls_amount,\n        cast(IMPROVEMENT_SURCHARGE as float) as improvement_surcharge,\n        cast(CONGESTION_SURCHARGE as float) as congestion_surcharge,\n        cast(AIRPORT_FEE as float) as airport_fee,\n        cast(CBD_CONGESTION_FEE as float) as cbd_congestion_fee,  -- Nueva columna 2025\n        -- Normalizar payment_type a valores legibles\n        case PAYMENT_TYPE\n            when 0 then 'Flex Fare trip'\n            when 1 then 'Credit card'\n            when 2 then 'Cash'\n            when 3 then 'No charge'\n            when 4 then 'Dispute'\n            when 5 then 'Unknown'\n            when 6 then 'Voided trip'\n        end as payment_type,\n        -- Mantener otras columnas\n        VENDORID as vendor_id,\n        PASSENGER_COUNT as passenger_count,\n        RATECODEID as rate_code_id,\n        STORE_AND_FWD_FLAG as store_and_fwd_flag,\n        PULocationID as pu_location_id,\n        DOLocationID as do_location_id\n    from raw\n),\n\ncleaned as (\n    select *\n    from standardized\n    where trip_distance >= 0\n      and fare_amount >= 0\n      and pickup_datetime is not null\n      and dropoff_datetime is not null\n      and pickup_datetime < dropoff_datetime\n      and trip_distance < 200  -- outlier razonable\n)\n\nselect *\nfrom cleaned", "file_path": "/home/src/scheduler/dbt/demo/models/staging/stg_yellow.sql", "language": "sql", "type": "dbt", "uuid": "dbt/demo/models/staging/stg_yellow"}, "/home/src/scheduler/dbt/demo/models/staging/stg_green.sql:dbt:sql:home/src/scheduler/dbt/demo/models/staging/stg green": {"content": "-- Modelo Silver: Yellow Taxi Trip, limpieza y estandarizaci\u00f3n\n{{ config(materialized='table') }}\n\nwith raw as (\n    select *\n    from {{ source('bronze', 'green_tripdata') }}\n),\n\nstandardized as (\n    select\n        -- Convertir fechas a timestamp y estandarizar zona horaria NYC\n        convert_timezone('America/New_York', LPEP_PICKUP_DATETIME) as pickup_datetime,\n        convert_timezone('America/New_York', LPEP_DROPOFF_DATETIME) as dropoff_datetime,\n        -- Convertir distancias y tarifas a float\n        cast(TRIP_DISTANCE as float) as trip_distance,\n        cast(FARE_AMOUNT as float) as fare_amount,\n        cast(TOTAL_AMOUNT as float) as total_amount,\n        cast(TIP_AMOUNT as float) as tip_amount,\n        cast(EXTRA as float) as extra,\n        cast(MTA_TAX as float) as mta_tax,\n        cast(TOLLS_AMOUNT as float) as tolls_amount,\n        cast(IMPROVEMENT_SURCHARGE as float) as improvement_surcharge,\n        cast(CONGESTION_SURCHARGE as float) as congestion_surcharge,\n        null as airport_fee,\n        cast(CBD_CONGESTION_FEE as float) as cbd_congestion_fee,  -- Nueva columna 2025\n        -- Normalizar payment_type a valores legibles\n        case PAYMENT_TYPE\n            when 0 then 'Flex Fare trip'\n            when 1 then 'Credit card'\n            when 2 then 'Cash'\n            when 3 then 'No charge'\n            when 4 then 'Dispute'\n            when 5 then 'Unknown'\n            when 6 then 'Voided trip'\n        end as payment_type,\n        -- Mantener otras columnas\n        VENDORID as vendor_id,\n        PASSENGER_COUNT as passenger_count,\n        RATECODEID as rate_code_id,\n        STORE_AND_FWD_FLAG as store_and_fwd_flag,\n        PULocationID as pu_location_id,\n        DOLocationID as do_location_id\n    from raw\n),\n\ncleaned as (\n    select *\n    from standardized\n    where trip_distance >= 0\n      and fare_amount >= 0\n      and pickup_datetime is not null\n      and dropoff_datetime is not null\n      and pickup_datetime < dropoff_datetime\n      and trip_distance < 200  -- outlier razonable\n)\n\nselect *\nfrom cleaned", "file_path": "/home/src/scheduler/dbt/demo/models/staging/stg_green.sql", "language": "sql", "type": "dbt", "uuid": "dbt/demo/models/staging/stg_green"}, "/home/src/scheduler/dbt/demo/models/marts/dim_date.sql:dbt:sql:home/src/scheduler/dbt/demo/models/marts/dim date": {"content": "{{ config(\n    materialized='table'\n) }}\n\nwith date_spine as (\n    -- Generar fechas para un rango razonable (asume que dbt_utils est\u00e1 instalado)\n    {{ dbt_utils.date_spine(\n        datepart=\"day\",\n        start_date=\"cast('2015-01-01' as date)\",\n        end_date=\"dateadd(year, 1, current_date())\"\n    ) }}\n),\n\nfinal as (\n    select\n        to_varchar(date_day, 'YYYYMMDD')::int as date_sk, \n        date_day as full_date,\n        dayname(date_day) as day_name,\n        dayofweek(date_day) as day_of_week,\n        monthname(date_day) as month_name,\n        year(date_day) as year,\n        case when dayofweek(date_day) in (6, 7) then true else false end as is_weekend\n    from date_spine\n)\n\nselect * from final", "file_path": "/home/src/scheduler/dbt/demo/models/marts/dim_date.sql", "language": "sql", "type": "dbt", "uuid": "dbt/demo/models/marts/dim_date"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}